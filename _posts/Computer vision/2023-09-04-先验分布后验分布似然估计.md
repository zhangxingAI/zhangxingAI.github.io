---

layout: post
title: 深入理解贝叶斯统计
category: 计算机视觉
tags: stable_diffusion VAE 
keywords: MLE MAP BE
typora-root-url: ../..

---

* TOC
{:toc}


### 贝叶斯统计

#### 后验概率（结果已知）

预先**已知结果**，然后根据结果**估计**（猜）**原因**的概率分布即 **后验概率**。
$$
P(因\mid果) \\
\\
P(\theta\mid X)
$$

#### 先验概率（由历史求因）

不基于当前预测事件的结果，而是根据历史规律确定**原因**的概率分布即 **先验概率**
$$
P(因) \\
\\
P(\theta)
$$

#### 似然估计（结果已知）

$$
P(果\mid因) \\
\\
P(X\mid\theta)
$$

> 对于函数 $P(x\mid\theta)$，从不同的观测角度来看可以分为以下两种情况：
>
> - 如果 $\theta$ 已知且保持不变， $x$ 是变量，则 $P(x\mid\theta)$ 称为概率函数，表示不同 $x$ 出现的概率。
> - 如果 $x$ 已知且保持不变， $\theta$ 是变量，则$P(x\mid\theta)$ 称为似然函数，表示不同 $\theta$ 下， $x$ 出现的概率，也记作$L(\theta\mid x)$ 或$L(x;\theta)$ 或$f(x;\theta)$ 。

#### 贝叶斯公式

$$
P(\theta \mid X)=\frac{P(X \mid \theta) * P(\theta)}{P(X)}
$$

$$
后验概率 =\frac{似然估计 * 先验概率}{evidence}
$$

$evidence$ 即为已发生的事实，是真实的证据，即为果。表示为：
$$
P(果) 
$$

#### 频率学派与贝叶斯学派

频率学派与贝叶斯学派探讨**不确定性**这件事时的出发点与立足点不同。频率统计认为事件服从特定的分布，分布的参数虽然未知但是固定。如果进行大量独立重复实验，那么事件发生的概率一定会趋向事件的真实概率。比如抛硬币实验，如果重复无数次的话，出面证明的概率会非常接近0.5。频率学派从**自然**角度出发，试图直接为**事件**本身建模，即**事件A**在独立重复试验中发生的频率趋于极限 $p$，那么这个极限就是该事件的概率。

频率统计非常依赖实验次数，比如抛硬币实验中，只进行了1000次实验，而这1000次实验中600次是正面。如果根据频率统计的观点，那么正面出现的概率应该是0.6。但是事实上，如果硬币没有问题的话，正面出现的概率应该是0.5。所以说此时（实验次数少）的情况下，频率统计的结果并不合理。

而贝叶斯统计的思想是加入先验知识来对观察的现象做出推断。比如说，我们可以假设正面出现的概率位于 [0.4, 0.6] *（ [0.4, 0.6]的区间假设就是先验知识）*的区间内。然后基于这个假设，去估计正面出现的概率。贝叶斯学派并不从试图刻画**事件**本身，而从**观察者**角度出发。贝叶斯学派并不试图说**事件本身是随机的**，或者**世界的本体带有某种随机性**，这套理论根本不言说关于**世界本体**的东西，而只是从**观察者知识不完备**这一出发点开始，构造一套在贝叶斯概率论的框架下可以对不确定知识做出推断的方法。

频率学派的代表是**最大似然估计**；贝叶斯学派的代表是**最大后验概率估计**。

#### 最大似然估计(MLE)/最大后验估计(MAP)/贝叶斯估计(BE)

> 给定一些数据样本 $x$，假定我们知道样本是从某一种分布中随机取出的，但我们不知道这个分布具体的参数 $\theta$。以抛硬币为例，假设我们有一枚硬币，现在要估计其正面朝上的概率$\theta$。为了对$\theta$进行估计，我们进行了10次实验（独立同分布，i.i.d.），这组实验记为$X=x_1，x_2，…，x_{10}$，其中正面朝上的次数为6次，反面朝上的次数为4次，结果为(1, 0, 1, 1, 0, 0, 0, 1, 1, 1)。

##### 最大似然估计

**最大似然估计**（MLE，Maximum Likelihood estimation）可以估计模型的参数。最大似然估计的思想是使得观测数据（样本）发生概率最大的参数就是最好的参数，换句话说，其目标是找出最佳参数 $ \theta$，使得模型产生出观测数据 $x$ 的概率最大：
$$
\underset{\theta}{\operatorname{argmax}} P(X \mid \theta)
$$
对一个独立同分布的样本集来说，总体的似然就是每个样本似然的乘积。针对抛硬币的问题，似然函数可写作：
$$
P(X\mid \theta)=L(X ; \theta)=\prod_{i=0}^n P\left(x_i \mid \theta\right)=\theta^6(1-\theta)^4
$$
根据最大似然估计，使$L(X;θ)$取得最大值的 $ \theta$ 即为估计结果，令
$$
\begin{aligned}
L(X;θ)^{'}&= 6θ^5(1-θ)^4-4θ^6(1-θ)^3  \\
& = θ^5(1-θ)^3(6-10θ) \\
& =0
\end{aligned}
$$
可得$\hat\theta =0.6$。似然函数图如下：

<img src="./public/upload/map/dis1.png" style="zoom:50%;" />

*由于总体的似然就是每个样本似然的乘积，为了求解方便，通常会将似然函数转成对数似然函数，然后再求解。可以转成对数似然函数的主要原因是对数函数并不影响函数的凹凸性。*

最大似然没有考虑先验知识，仅凭样本数据进行预估，在样本量小的时候容易产生过拟合。在上述的抛硬币case中，最大似然估计认为使似然函数 $P(X\mid\theta)$ 最大的参数 $θ$ 即为最好的 $θ$ ，此时最大似然估计是将 $θ$ 看作固定的值，只是其值未知。

##### 最大后验概率估计

**最大后验概率**估计（Maximum A Posteriori Estimation）分布认为$θ$ 是一个随机变量，即$θ$ 具有某种概率分布，称为先验分布，求解时除了要考虑似然函数 $P(X\mid\theta)$ 之外，还要考虑$θ$ 的先验分布 $P(θ)$，因此其认为使 $P(X\mid\theta)P(\theta)$ 取最大值的$θ$ 就是最好的$θ$ 。此时要最大化的函数变为$P(X\mid\theta)P(\theta)$ ，由于$X$的先验分布$P(X)$是固定的（可通过分析数据获得，其实我们也不关心$X$的分布，我们关心的是$θ$ ），因此最大化函数可变为$\frac {P(X\mid\theta)P(\theta)}{P(X)}$ ，根据贝叶斯法则，要最大化的函数$\frac {P(X\mid\theta)P(\theta)}{P(X)} = P(\theta\|X)$，因此要最大化的函数是 $P(\theta\|X)$，而 $P(\theta\|X)$是 $θ$ 的后验概率。最大后验概率估计可以看作是正则化的最大似然估计，当然机器学习或深度学习中的正则项通常是加法，而在最大后验概率估计中采用的是乘法， $P(\theta)$是正则项。在最大似然估计中，由于认为 $\theta$ 是固定的，因此 $P(\theta)=1$。

最大后验概率估计的公式表示：
$$
\underset{\theta}{\operatorname{argmax}} P(\theta \mid X) = \underset{\theta}{\operatorname{argmax}}\frac{P(X \mid \theta)P(\theta)}{P(X)}
$$
因为给定样本 $x$ 后， $P(X)$ 在 $\theta$ 空间上为一个定值，和 $\theta$的大小没有关系，所以可以省略分母 $p(X)$。
可化简为:
$$
\underset{\theta}{\operatorname{argmax}} P(\theta \mid X) = \underset{\theta}{\operatorname{argmax}}{P(X \mid \theta)P(\theta)}
$$
即为：
$$
\text { Posterior } \propto(\text { Likelihood } * \text { Prior })
$$
在抛硬币的例子中，通常认为$θ=0.5$ 的可能性最大，因此我们用均值为$0.5$，方差为$0.1$的高斯分布来描述$θ$ 的先验分布，当然也可以使用其它的分布来描述 $θ$ 的先验分布。$θ$ 的先验分布为：
$$
P(\theta) = \frac{1}{\sqrt{2 \pi \sigma}} e^{-\frac{(\theta-\mu)^2}{2 \sigma^2}}=\frac{1}{10 \sqrt{2 \pi}} e^{-50(\theta-0.5)^2}
$$
在最大似然估计中，已知似然函数为 $P(X\mid \theta)=\theta^6(1-\theta)^4$，因此要求$P(X \mid \theta)P(\theta)$的最大值，则 
$$
\operatorname {ln}^{'}(P(X \mid \theta)P(\theta))=0
$$
得
$$
100 \theta^3-150 \theta^2+40 \theta+6=0
$$
由于 $0\leqslant \theta \leqslant 1$ ，解得 $\hat\theta =0.529$。

如果我们用均值为$0.6$，方差为$0.1$的高斯分布来描述$θ$ 的先验分布，则$\hat\theta =0.6$。由此可见，在最大后验概率估计中，$θ$ 的估计值与$θ $ 的先验分布有很大的关系。这也说明一个合理的先验概率假设是非常重要的。如果先验分布假设错误，则会导致估计的参数值偏离实际的参数值。

一般说来，先验分布 $p(θ)$ 是反映人们在抽样前对 $\theta$ 的认识，后验分布 $P(θ\mid X)$ 是反映人们在抽样后对 $\theta$ 的认识，之间的差异是由于样本的出现后人们对$\theta$ 认识的一种调整，所以**后验分布 $P(θ\mid X)$ 可以看作是人们用总体信息和样本信息（抽样信息）对先验分布 $P(θ)$ 作调整的结果**。

##### 贝叶斯估计

贝叶斯估计，假定把待估计的参数看成是符合某种先验概率分布的随机变量，而不是确定数值。在样本分布上，计算参数所有可能的情况，并通过计算参数的期望，得到后验概率密度。

贝叶斯统计的重要特点在于，我们在建模前需要给出模型参数 � 的**先验分布（prior distribution）** �(�) 。也就是说，在得到任何数据，或者将任何数据对模型进行拟合之前，我们需要先给定模型参数服从的分布。例如，对于抛掷一枚硬币一次可能出现的结果，我们可以构建一个参数为 � 的伯努利分布概率模型 �∼���������(�) 。那么在使用数据估计参数 � 之前，我们需要给这个参数设定一个分布（**注意，先验分布是关于模型参数的分布，而不是我们建模的对象本身，下文中将要介绍的后验分布，也是关于模型参数的分布）。**给出参数的先验分布，是贝叶斯统计的核心部分之一，也是对于长期接触频率学派思想的贝叶斯初学者来说最容易困惑的部分：**没有数据的支持，在贝叶斯统计中的先验分布到底如何设定？**

